
## Modelling texts (again)


- *n-grams*: a *sequence* of tokens as a feature 

(Wait: what's a *token*?)



--- 

## A Pluto notebook looking at n-gram frequencies

New information!


- about individual *languages*
- about individual *versions of texts*

An analogy: 

> - amino acid sequences are not random!
> - sequence -> function

---

## Modeling sequences: "word vectors"

- what feature is followed by what?
- look at each ngram and see what comes next
- count frequencies (likelihoods)

You have a predictive model!

---

## Hugh Kenner, "Travesty generator"

*Byte* magazine  [in 1984!](https://archive.org/details/byte-magazine-1984-11/page/n129/mode/2up)!


---

## Write a text generator in 2023


Predictive models based on  n-gram frequencies:

- cheapΓΠΤ
- cheapGPT


---

## ChatGPT

---